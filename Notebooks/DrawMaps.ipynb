{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw regions found by the metrics in the map\n",
    "This notebook is used to obtain the map drawings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.sparse import coo_matrix,csr_matrix,save_npz,load_npz\n",
    "%matplotlib widget\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.colors as mpc\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "cimport cython\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lng1, lat1, lng2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lng1, lat1, lng2, lat2 = map(radians, [lng1, lat1, lng2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlng = abs(lng2 - lng1)\n",
    "    if dlng > np.pi: #warp around longitude\n",
    "        dlng = 2*np.pi-dlng\n",
    "    dlat = abs(lat2 - lat1)\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlng/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def manhatton(lng1, lat1, lng2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance between two points \n",
    "    on the cylinder (all gird distance are transformed into equator scale)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lng1, lat1, lng2, lat2 = map(radians, [lng1, lat1, lng2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlng = abs(lng2 - lng1)\n",
    "    if dlng > np.pi: #warp around longitude\n",
    "        dlng = 2*np.pi-dlng\n",
    "    dlat = abs(lat2 - lat1)\n",
    "    c = dlng + dlat\n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def geoKernel_cython(float[:,::1] data, float threshold):\n",
    "    \n",
    "    n = len(data)\n",
    "    X = np.empty((n,n), dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            # call the Haversine formula\n",
    "            dist = manhatton(data[i][0],data[i][1],data[j][0],data[j][1])\n",
    "            if (dist <= threshold)&(dist>0.1):\n",
    "                X[i][j] = dist\n",
    "            else:\n",
    "                X[i][j] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads or recalculates and store geokernel\n",
    "# set reCalculateGeoKernel to True to recalculate GeoKernel\n",
    "reCalculateGeoKernel = False\n",
    "if (reCalculateGeoKernel):\n",
    "    dfCorrelation = pd.read_csv('../Data/LaggedCorrelation_d7_l90_v3.csv', sep=',')\n",
    "    pointsCorrelation = gpd.GeoDataFrame(dfCorrelation, geometry=gpd.points_from_xy(dfCorrelation.lng, dfCorrelation.lat))\n",
    "  \n",
    "    data = pointsCorrelation[['lng','lat']].to_numpy()\n",
    "    data = data.copy(order='C').astype('float32') \n",
    "    data.flags\n",
    "\n",
    "    N, thresh = len(data), 200\n",
    "    X = geoKernel_cython(data, thresh)\n",
    "\n",
    "    X[X<0.0000001] = 0\n",
    "    Xs = X + X.T - np.diag(X.diagonal())\n",
    "    fullGeo = coo_matrix(Xs)\n",
    "    save_npz('../Data/geokernel.npz', fullGeo)\n",
    "else:\n",
    "    fullGeo = load_npz(\"../Data/geokernel.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient sparse geo-weight-dist hadamard-product with input data. Notice this directly cacluates a distance matrix instead of a kernel\n",
    "def geoWeightedDist(data, geoDist, smoothness):\n",
    "    # creating masks for input data given the geo-kernel\n",
    "    value_i = data[geoDist.row]\n",
    "    value_j = data[geoDist.col]\n",
    "    # only calculate between points that is non-zero in geo-kernel\n",
    "    #Xd = np.subtract(value_i,value_j)\n",
    "    Xshrink = np.multiply(value_i,value_j).reshape(len(value_i),)#Shrinking factor based on value, smaller the value, closer the distance\n",
    "    #Xd = np.exp(Xd**2/smoothness**2).reshape(len(value_i),)\n",
    "    #geoSmooth = geoDist.copy()\n",
    "    Xd = np.exp(geoDist.data**2*np.power(Xshrink,smoothness)/geoDist.data.std()**2)\n",
    "    #Xd = np.multiply(Xd,geoSmooth.data)\n",
    "    C = coo_matrix((Xd/max(Xd), (geoDist.row, geoDist.col)), shape=(len(data),len(data)))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drawNewMap(filename = 'Data/Correlation_d7_l180_v3.csv',percentile = 0.30,pvalueThreshold = None,smoothness = 0.5, quantile = 0.5, extraEpsilonFactor = 0.5, min_samples =5, min_points = 20,exportName=\"\",useCircular=False,maxPositivePValue = None):\n",
    "    dfCorrelation = pd.read_csv(filename, sep=',')\n",
    "  \n",
    "    gridRegions = [];\n",
    "    for coords in dfCorrelation[[\"lat\",\"lng\"]].to_numpy():\n",
    "        region = (coords[0] - 0.5, coords[0]+0.5,coords[1]-0.625,coords[1]+0.625)\n",
    "        gridRegions.append(region)\n",
    "  \n",
    "  \n",
    "    dfCorrelation[\"region\"] = gridRegions;\n",
    "  \n",
    "  \n",
    "    # pointsCorrelation = gpd.GeoDataFrame(dfCorrelation, geometry=gpd.points_from_xy(dfCorrelation.lng, dfCorrelation.lat))\n",
    "    pointsCorrelation = dfCorrelation.reset_index(drop=True)\n",
    "  \n",
    "    if(percentile<1.0):\n",
    "        thresholdCorrelation = np.quantile(pointsCorrelation[['pvalue']],percentile);\n",
    "    else:\n",
    "        thresholdCorrelation = 1.0\n",
    "      \n",
    "    if(pvalueThreshold is not None):\n",
    "        thresholdCorrelation = pvalueThreshold;\n",
    "      \n",
    "    pointsCorrelation['log-pvalue'] = np.log10(pointsCorrelation['pvalue'])\n",
    "    pointsCorrelation['thresh-pvalue'] = pointsCorrelation['pvalue']\n",
    "  \n",
    "  \n",
    "    pointsCorrelation.loc[pointsCorrelation['thresh-pvalue']>thresholdCorrelation,'thresh-pvalue']=1\n",
    "  \n",
    "    points = pointsCorrelation;\n",
    "  \n",
    "    print(\"Chosen p-value: %.2g\"%thresholdCorrelation);\n",
    "    print(\"Chosen correlation: %.2g\"%np.max(pointsCorrelation.loc[pointsCorrelation['thresh-pvalue']>thresholdCorrelation,'value']));\n",
    "  \n",
    "    tab20b = cm.get_cmap('tab20b', 20)\n",
    "    tab20c = cm.get_cmap('tab20c', 20)\n",
    "    newcmp = mpc.ListedColormap(tab20b(np.linspace(0.25, 0.75, 256)))\n",
    "    a = np.concatenate([tab20b(range(20)),tab20c(range(20))])\n",
    "    np.random.shuffle(a)\n",
    "    newcmp = mpc.ListedColormap(a)\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    def identity_scale(minval, maxval):\n",
    "        def scalar(val):\n",
    "            return 2\n",
    "        return scalar\n",
    "  \n",
    "    # pthreshold=0.05\n",
    "  \n",
    "    if(quantile<1.0):\n",
    "        plotGraph = geoWeightedDist(points[['thresh-pvalue']].to_numpy(),fullGeo,smoothness)\n",
    "        clustering = DBSCAN(eps=np.quantile(plotGraph.data, quantile)*extraEpsilonFactor, min_samples=min_samples, metric='precomputed').fit(plotGraph)\n",
    "        labels = clustering.labels_\n",
    "  \n",
    "        smallerGroups = set();\n",
    "        allLabels = list(map(str,(labels)));\n",
    "        for label,count in Counter(allLabels).items():\n",
    "            if(count<min_points):\n",
    "                smallerGroups.add(label)\n",
    "  \n",
    "        #labels2 = spectral_clustering(graph2, n_clusters=10, eigen_solver='arpack')\n",
    "        # plt.hist(labels, normed=True, bins=30)\n",
    "        toPlotPoints = points.copy()\n",
    "        toPlotPoints['labels'] = [entry if entry not in smallerGroups else \"-1\" for entry in allLabels]\n",
    "        toPlotData = toPlotPoints[(toPlotPoints[\"labels\"]!=\"-1\") * (toPlotPoints[\"thresh-pvalue\"]<thresholdCorrelation)]\n",
    "    else:\n",
    "        toPlotData = points.copy();\n",
    "  \n",
    "  \n",
    "    ENSORegion = [[-5 , -170] ,[5, -120]];\n",
    "  \n",
    "    cdict1 = {'red':   ((0.0, 0.0, 0.0),\n",
    "                       (0.5, 0.5, 1.0),\n",
    "                       (1.0, 1.0, 0.0)),\n",
    "  \n",
    "             'green': ((0.0, 0.0, 0.0),\n",
    "                       (0.5, 1.0, 1.0),\n",
    "                       (1.0, 0.0, 0.0)),\n",
    "  \n",
    "             'blue':  ((0.0, 0.0, 1.0),\n",
    "                       (0.5, 1.0, 0.0),\n",
    "                       (1.0, 0.0, 0.0))\n",
    "            }\n",
    "  \n",
    "    bluered = matplotlib.cm.get_cmap(\"autumn\");#LinearSegmentedColormap(\"bluered\", cdict1)\n",
    "  \n",
    "    if(maxPositivePValue is None):\n",
    "        maxPositivePValue = np.min(toPlotData[\"log-pvalue\"][np.isfinite(toPlotData[\"log-pvalue\"])])\n",
    "    print(maxPositivePValue)\n",
    "  #   maxPositivePValue = -40\n",
    "  \n",
    "    if(True):\n",
    "        if(useCircular):\n",
    "            projection = ccrs.Mollweide(central_longitude=150)\n",
    "        else:\n",
    "            projection = ccrs.PlateCarree(central_longitude=150);\n",
    "  \n",
    "        fig = plt.figure(figsize=(10,8),tight_layout=True)\n",
    "        ax = plt.axes(projection=projection)\n",
    "        # ax.set_extent([-30, 150, -70, 70])\n",
    "        # ax.stock_img()\n",
    "        ax.set_global()\n",
    "        if(not useCircular):\n",
    "            ax.set_extent([-30, 150, -70, 70])\n",
    "        ax.add_feature(cfeature.LAND,facecolor='gray',alpha=1.0)\n",
    "        ax.add_feature(cfeature.OCEAN,facecolor=(0.7,0.7,0.7),alpha=1.0)\n",
    "        # ax.add_feature(cfeature.OCEAN)\n",
    "  \n",
    "      #Arrows lines \n",
    "  \n",
    "        transform = projection._as_mpl_transform(ax)\n",
    "      \n",
    "        def plotRectangle(ax,region,name,color,transform = ccrs.Geodetic()):\n",
    "            if(transform):\n",
    "                ax.add_patch(mpatches.Rectangle(xy=[region[2], region[0]], width=(region[3]-region[2]), height=(region[1]-region[0]),\n",
    "                                              fc=color,\n",
    "                                              ec=(0.1,0.1,0.1,1.0),\n",
    "                                              lw=0.1,\n",
    "                                              transform=transform))\n",
    "            else:\n",
    "                ax.add_patch(mpatches.Rectangle(xy=[region[2], region[0]], width=(region[3]-region[2]), height=(region[1]-region[0]),\n",
    "                                              fc=color,\n",
    "                                              lw=0.1,\n",
    "                                              ec=(0.1,0.1,0.1,1.0)))\n",
    "            if(name):\n",
    "                ax.text((region[3]+region[2])*0.5, (region[1]), name,horizontalalignment='center',verticalalignment='bottom', transform=ccrs.Geodetic(),zorder=6)\n",
    "  \n",
    "      \n",
    "        norm = mpl.colors.Normalize(vmin=maxPositivePValue, vmax=np.log10(thresholdCorrelation))\n",
    "      \n",
    "        for region,pvalue in  toPlotData[[\"region\",\"thresh-pvalue\"]].to_numpy():\n",
    "            regionPvalue = pvalue;\n",
    "            regionPvalueLog = np.log10(regionPvalue);\n",
    "            lat,lng = np.mean(region[0:1]),np.mean(region[2:3]);\n",
    "            if(np.isfinite(regionPvalueLog) and regionPvalueLog>=maxPositivePValue):\n",
    "                color=bluered(norm(regionPvalueLog));\n",
    "            else:\n",
    "                color=(0.5,0.0,0.0,1.0);\n",
    "  \n",
    "            plotRectangle(ax,region,\"\",color,transform=ccrs.Geodetic());\n",
    "  \n",
    "        plotRectangle(ax,[-5 , 5 ,-170, -120],\"ENSO\",\"#CCCCCC\",transform=ccrs.Geodetic());\n",
    "  \n",
    "  \n",
    "        ax.add_feature(cfeature.COASTLINE,edgecolor='black',alpha=0.5)\n",
    "        ax.add_feature(cfeature.BORDERS,edgecolor='black',alpha=0.5)\n",
    "  \n",
    "  \n",
    "        import json\n",
    "  \n",
    "        with open(\"../Data/ElNidoShapes.json\",\"r\") as fd:\n",
    "            shapes = [shape[0] for shape in json.load(fd)];\n",
    "  \n",
    "        with open(\"../Data/LaNinaShapes.json\",\"r\") as fd:\n",
    "            shapes += [shape[0] for shape in json.load(fd)];\n",
    "  \n",
    "  \n",
    "  \n",
    "        shapesData = [np.array(shape) for shape in shapes]  \n",
    "  \n",
    "        for i in range(len(shapesData)):\n",
    "            polygon = mpatches.Polygon(shapesData[i],\n",
    "              closed=False,\n",
    "              facecolor='white',\n",
    "              alpha=0.2,\n",
    "              transform=ccrs.Geodetic())\n",
    "            ax.add_patch(polygon)\n",
    "  \n",
    "  \n",
    "  \n",
    "        print(maxPositivePValue);\n",
    "        if(exportName):\n",
    "            fig.savefig(\"Figures/\"+exportName+\".pdf\");\n",
    "        plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 1))\n",
    "    fig.subplots_adjust(bottom=0.5)\n",
    "    \n",
    "    def fmt(x, pos):\n",
    "        return '$10^{%d}$'%(x);\n",
    "  \n",
    "    if(-maxPositivePValue<10):\n",
    "        bounds=np.arange(np.ceil(maxPositivePValue),round(np.log10(thresholdCorrelation))+1,1)\n",
    "    elif(-maxPositivePValue<50):\n",
    "        bounds=np.arange(np.ceil(maxPositivePValue),round(np.log10(thresholdCorrelation))+1,5)\n",
    "    elif(-maxPositivePValue<100):\n",
    "        bounds=np.arange(np.ceil(maxPositivePValue),round(np.log10(thresholdCorrelation))+1,10)\n",
    "    elif(-maxPositivePValue<200):\n",
    "        bounds=np.arange(np.ceil(maxPositivePValue),round(np.log10(thresholdCorrelation))+1,20)\n",
    "    else:\n",
    "        bounds=np.arange(np.ceil(maxPositivePValue),round(np.log10(thresholdCorrelation))+1,50)\n",
    "      \n",
    "    print(bounds);\n",
    "    ccmap = matplotlib.cm.get_cmap(\"autumn\")\n",
    "    ccmap.set_under(color=(0.5,0.0,0.0,1.0))\n",
    "    norm = mpl.colors.Normalize(vmin=maxPositivePValue, vmax=np.log10(thresholdCorrelation))\n",
    "    cb1 = mpl.colorbar.ColorbarBase(ax, cmap=ccmap,\n",
    "                                    norm=norm,\n",
    "  #                                   boundaries=[-maxPositivePValue] + bounds + [maxPositivePValue],\n",
    "                                    extend='min',\n",
    "                                    extendfrac=0.05,\n",
    "                                    spacing='uniform',\n",
    "                                    ticks=bounds,\n",
    "                                    orientation='horizontal',\n",
    "                                   format=ticker.FuncFormatter(fmt))\n",
    "  \n",
    "    if(exportName):\n",
    "        plt.savefig(\"Figures/cm_%s.pdf\"%exportName)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plotConfigs = [\n",
    "# (\"LaggedCorrelation\",0,None),\n",
    "# (\"LaggedCorrelation\",30,None),\n",
    "# (\"LaggedCorrelation\",90,None),\n",
    "# (\"LaggedCorrelation\",180,None),\n",
    "# (\"Causality\",30,-20),\n",
    "# (\"Causality\",90,-20),\n",
    "(\"Causality\",180,-20)\n",
    "]\n",
    "\n",
    "windowDays = 7\n",
    "maxLag = 90\n",
    "propertyName = \"LaggedCorrelationGeneral\"\n",
    "for propertyName,maxLag,maxPositivePValue in plotConfigs:\n",
    "    drawNewMap('../Data/%s_d%d_l%d_v3.csv'%(propertyName,windowDays,maxLag),\n",
    "          smoothness = 0.5, \n",
    "#           percentile = 1.0,# 0.2\n",
    "          pvalueThreshold=0.01,\n",
    "          quantile = 0.5,\n",
    "          extraEpsilonFactor = 0.5,\n",
    "          min_samples = 5,\n",
    "          min_points = 30,\n",
    "#           exportName=\"region_%s_d%d_l%d_v3_complete\"%(propertyName,windowDays,maxLag),\n",
    "          maxPositivePValue=maxPositivePValue,\n",
    "         );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:climate]",
   "language": "python",
   "name": "conda-env-climate-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
