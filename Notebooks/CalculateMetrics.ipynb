{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Lagged correlation and Granger causality\n",
    "This notebook calculates the lagged correlation and Granger causality for the input data. It exports them to Data folder, which can be used in the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pyunicorn.timeseries.surrogates as pysurrogates\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import bgzf;\n",
    "import struct;\n",
    "\n",
    "#Desired Lon (nx) and Lat (ny)\n",
    "nx = 288\n",
    "ny = 140\n",
    "# nx = 36\n",
    "# ny = 14\n",
    "\n",
    "# Maximum lag\n",
    "lagDays = 120\n",
    "\n",
    "dataRange = (0,100) # Cut the time series according to percentage\n",
    "\n",
    "# time resolution range\n",
    "resolutionRange = range(7,31)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load files in the bgz format (see ParseData to adapt it to use your own data)\n",
    "def loadCompressedClimateData():\n",
    "    with bgzf.open(\"../Data/climateData.bgz\",\"rb\") as fd:\n",
    "        data = fd.read(8*2);\n",
    "        gridCount,seriesSize = struct.unpack(\"<QQ\",data);\n",
    "        latitudes = np.zeros(gridCount,dtype=np.float32);\n",
    "        longitudes = np.zeros(gridCount,dtype=np.float32);\n",
    "        temperatures = np.zeros((gridCount,seriesSize),dtype=np.float32);\n",
    "        preciptations = np.zeros((gridCount,seriesSize),dtype=np.float32);\n",
    "        for i in range(gridCount):\n",
    "            data = fd.read(4*2);\n",
    "            lat,lon = struct.unpack(\"<ff\",data);\n",
    "            latitudes[i] = lat;\n",
    "            longitudes[i] = lon;\n",
    "            data = fd.read(4*seriesSize);\n",
    "            temperatures[i,:] = struct.unpack(\"<%df\"%seriesSize,data);\n",
    "            data = fd.read(4*seriesSize);\n",
    "            preciptations[i,:] = struct.unpack(\"<%df\"%seriesSize,data);\n",
    "    return latitudes,longitudes,temperatures,preciptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data from the bgz file\n",
    "latitudes,longitudes,temperaturesOrig,preciptationsOrig = loadCompressedClimateData();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "#%%cython --annotate\n",
    "# distutils: extra_compile_args=-fopenmp -O3\n",
    "# distutils: extra_link_args=-fopenmp -O3\n",
    "\n",
    "import numpy as np\n",
    "cimport cython\n",
    "from cython.parallel import prange\n",
    "from libc.math cimport sqrt\n",
    "from libc.math cimport fabs\n",
    "\n",
    "ctypedef fused aNumber:\n",
    "    float\n",
    "    double\n",
    "    \n",
    "# Function to mask regions\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "def maskForRegion(aNumber[:] boundingBox, aNumber[:] longitudes, aNumber[:] latitudes):\n",
    "    mask = [];\n",
    "    cdef Py_ssize_t pointCount;\n",
    "    pointCount = len(latitudes);\n",
    "    for i in range(pointCount):\n",
    "        if((boundingBox[0]==boundingBox[1] or (latitudes[i] < boundingBox[1] and latitudes[i] > boundingBox[0])) \\\n",
    "            and (boundingBox[2]==boundingBox[3] or (longitudes[i] < boundingBox[3] and longitudes[i] > boundingBox[2]))):\n",
    "            mask.append(i);\n",
    "    return np.array(mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "#--annotate\n",
    "# %%cython\n",
    "# distutils: extra_compile_args=-fopenmp -O3\n",
    "# distutils: extra_link_args=-fopenmp -O3\n",
    "\n",
    "\n",
    "\n",
    "#### A-cython: profile=True\n",
    "#### A-cython: linetrace=True\n",
    "#### A-cython: binding=True\n",
    "#### A-distutils: define_macros=CYTHON_TRACE_NOGIL=1\n",
    "\n",
    "import numpy as np\n",
    "cimport cython\n",
    "from cython.parallel import prange\n",
    "from libc.math cimport sqrt\n",
    "from libc.math cimport fabs\n",
    "\n",
    "import pyunicorn.timeseries.surrogates as pysurrogates\n",
    "\n",
    "ctypedef fused aNumber:\n",
    "    float\n",
    "    double\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef aNumber computeAverageTo(aNumber[:, ::1] data, Py_ssize_t entryIndex, Py_ssize_t featuresCount) nogil:\n",
    "    cdef aNumber average;\n",
    "    cdef Py_ssize_t featureIndex;\n",
    "    average=0;\n",
    "    for featureIndex in range(featuresCount):\n",
    "        average+=data[entryIndex,featureIndex];\n",
    "    return average/featuresCount;\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef aNumber computeStdDevTo(aNumber[:, ::1] data, aNumber average,Py_ssize_t entryIndex, Py_ssize_t featuresCount) nogil:\n",
    "    cdef aNumber stddev;\n",
    "    cdef Py_ssize_t featureIndex;\n",
    "    cdef aNumber value;\n",
    "    stddev=0;\n",
    "    for featureIndex in range(featuresCount):\n",
    "        value = data[entryIndex,featureIndex]-average;\n",
    "        stddev+=value**2;\n",
    "    return sqrt(stddev/featuresCount);\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef aNumber computeAverageFrom(aNumber[:] data, Py_ssize_t featuresCount) nogil:\n",
    "    cdef aNumber average;\n",
    "    cdef Py_ssize_t featureIndex;\n",
    "    average=0;\n",
    "    for featureIndex in range(featuresCount):\n",
    "        average+=data[featureIndex];\n",
    "    return average/featuresCount;\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef aNumber computeStdDevFrom(aNumber[:] data, aNumber average, Py_ssize_t featuresCount) nogil:\n",
    "    cdef aNumber stddev;\n",
    "    cdef Py_ssize_t featureIndex;\n",
    "    cdef aNumber value;\n",
    "    stddev=0;\n",
    "    for featureIndex in range(featuresCount):\n",
    "        value = data[featureIndex]-average;\n",
    "        stddev+=value**2;\n",
    "    return sqrt(stddev/featuresCount);\n",
    "\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "def computeLagCorrelations(aNumber[:] dataFrom, aNumber[:, ::1] dataTo, Py_ssize_t maxLag):\n",
    "    cdef Py_ssize_t entriesToCount = dataTo.shape[0]\n",
    "    cdef Py_ssize_t start = maxLag\n",
    "    cdef Py_ssize_t w = dataFrom.shape[0]-start\n",
    "    \n",
    "    \n",
    "    assert dataTo.shape[1] == dataFrom.shape[0]\n",
    "    \n",
    "    if aNumber is double:\n",
    "        dtype = np.double\n",
    "    elif aNumber is float:\n",
    "        dtype = float\n",
    "    \n",
    "    correlations = np.zeros(entriesToCount, dtype=dtype)\n",
    "    lags = np.zeros(entriesToCount, dtype=np.intc)\n",
    "    \n",
    "    cdef aNumber[:] correlations_view = correlations\n",
    "    cdef int[:] lags_view = lags\n",
    "\n",
    "    cdef aNumber fromAverage # = np.average(dataFrom[:w]);\n",
    "    cdef aNumber fromStdDev # = np.std(dataFrom[:w]);\n",
    "    \n",
    "    cdef aNumber toAverage\n",
    "    cdef aNumber toStdDev\n",
    "    cdef Py_ssize_t toIndex\n",
    "    \n",
    "    cdef int lag\n",
    "    \n",
    "    \n",
    "    cdef int featureIndex\n",
    "    cdef aNumber correlation\n",
    "    cdef aNumber maxCorrelation\n",
    "    cdef int lagMax\n",
    "    \n",
    "    for toIndex in prange(entriesToCount, nogil=True):#\n",
    "        maxCorrelation=0;\n",
    "        lagMax=0;\n",
    "        toAverage = computeAverageTo(dataTo[:,(start):],toIndex, w);\n",
    "        toStdDev = computeStdDevTo(dataTo[:,(start):],toAverage,toIndex,w);\n",
    "\n",
    "        for lag in range(maxLag+1):\n",
    "            fromAverage = computeAverageFrom(dataFrom[(start-lag):], w);\n",
    "            fromStdDev = computeStdDevFrom(dataFrom[(start-lag):],fromAverage,w);\n",
    "            correlation = 0\n",
    "            for featureIndex in range(w):\n",
    "                correlation = correlation + (dataFrom[featureIndex+start-lag]-fromAverage)*(dataTo[toIndex,featureIndex+(start)]-toAverage);\n",
    "            correlation = correlation/toStdDev/fromStdDev/w;\n",
    "#             print((fromAverage,toAverage,fromStdDev,toStdDev,w,correlation))\n",
    "    #                 print(\"[%d,%d] %d:%.2f (max:%.2f)\"%(fromIndex,toIndex,lag,correlation,maxCorrelation));\n",
    "            if(fabs(maxCorrelation)<fabs(correlation)):\n",
    "                maxCorrelation=correlation;\n",
    "                lagMax=lag;\n",
    "\n",
    "        correlations_view[toIndex] = maxCorrelation;\n",
    "        lags_view[toIndex] = lagMax;\n",
    "\n",
    "    return (correlations,lags)\n",
    "\n",
    "\n",
    "\n",
    "#Compute Lag Correlation\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "def computeLagCorrelations(aNumber[:] dataFrom, aNumber[:, ::1] dataTo, Py_ssize_t maxLag):\n",
    "    cdef Py_ssize_t entriesToCount = dataTo.shape[0]\n",
    "    cdef Py_ssize_t start = maxLag\n",
    "    cdef Py_ssize_t w = dataFrom.shape[0]-start\n",
    "    \n",
    "    \n",
    "    assert dataTo.shape[1] == dataFrom.shape[0]\n",
    "    \n",
    "    if aNumber is double:\n",
    "        dtype = np.double\n",
    "    elif aNumber is float:\n",
    "        dtype = float\n",
    "    \n",
    "    correlations = np.zeros(entriesToCount, dtype=dtype)\n",
    "    lags = np.zeros(entriesToCount, dtype=np.intc)\n",
    "    \n",
    "    cdef aNumber[:] correlations_view = correlations\n",
    "    cdef int[:] lags_view = lags\n",
    "\n",
    "    cdef aNumber fromAverage # = np.average(dataFrom[:w]);\n",
    "    cdef aNumber fromStdDev # = np.std(dataFrom[:w]);\n",
    "    \n",
    "    cdef aNumber toAverage\n",
    "    cdef aNumber toStdDev\n",
    "    cdef Py_ssize_t toIndex\n",
    "    \n",
    "    cdef int lag\n",
    "    \n",
    "    \n",
    "    cdef int featureIndex\n",
    "    cdef aNumber correlation\n",
    "    cdef aNumber maxCorrelation\n",
    "    cdef int lagMax\n",
    "    \n",
    "    for toIndex in prange(entriesToCount, nogil=True):#\n",
    "        maxCorrelation=0;\n",
    "        lagMax=0;\n",
    "        toAverage = computeAverageTo(dataTo[:,(start):],toIndex, w);\n",
    "        toStdDev = computeStdDevTo(dataTo[:,(start):],toAverage,toIndex,w);\n",
    "\n",
    "        for lag in range(maxLag+1):\n",
    "            fromAverage = computeAverageFrom(dataFrom[(start-lag):], w);\n",
    "            fromStdDev = computeStdDevFrom(dataFrom[(start-lag):],fromAverage,w);\n",
    "            correlation = 0\n",
    "            for featureIndex in range(w):\n",
    "                correlation = correlation + (dataFrom[featureIndex+start-lag]-fromAverage)*(dataTo[toIndex,featureIndex+(start)]-toAverage);\n",
    "            correlation = correlation/toStdDev/fromStdDev/w;\n",
    "#             print((fromAverage,toAverage,fromStdDev,toStdDev,w,correlation))\n",
    "    #                 print(\"[%d,%d] %d:%.2f (max:%.2f)\"%(fromIndex,toIndex,lag,correlation,maxCorrelation));\n",
    "            if(fabs(maxCorrelation)<fabs(correlation)):\n",
    "                maxCorrelation=correlation;\n",
    "                lagMax=lag;\n",
    "\n",
    "        correlations_view[toIndex] = maxCorrelation;\n",
    "        lags_view[toIndex] = lagMax;\n",
    "\n",
    "    return (correlations,lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def averageTimeseriesForRegions(regions,timeSeries,longitudes,latitudes,w=0,):\n",
    "    averageSeries = [];\n",
    "    errorSeries = [];\n",
    "    for region in tqdm(regions):\n",
    "        mask = maskForRegion(np.array(region,dtype=float),longitudes,latitudes);\n",
    "#         print(len(mask))\n",
    "        averageEntry = np.average(timeSeries[mask],axis=0);\n",
    "        errorEntry = np.std(timeSeries[mask],axis=0);\n",
    "        if(w>0):\n",
    "            averageEntry = moving_average(averageEntry,w);\n",
    "            errorEntry = moving_average(errorEntry,w);\n",
    "        averageSeries.append(averageEntry); \n",
    "        errorSeries.append(errorEntry);\n",
    "    return averageSeries,errorSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleTimeSeries(inputSeries):\n",
    "    a = np.array(inputSeries.T);\n",
    "    np.random.shuffle(a)\n",
    "    return a.T;\n",
    "\n",
    "def calcCausalityBivariate(X, causedByIndex, causingIndex, order): #Significance level alpha is generally 0.05\n",
    "    causedByIndex = causedByIndex\n",
    "    causingIndex = causingIndex\n",
    "    samplesCount = X.shape[0]\n",
    "    dataLength = X.shape[1]\n",
    "\n",
    "    causedNew = X[order:, causedByIndex]\n",
    "\n",
    "    completeX = np.empty((samplesCount - order, 0))\n",
    "    for lagIndex in range(order):\n",
    "        completeX = np.hstack((completeX, X[lagIndex : samplesCount - order + lagIndex]))\n",
    "\n",
    "    reducedX = np.delete(X, causingIndex, 1)\n",
    "\n",
    "    reducedXcomplete = np.empty((samplesCount - order, 0))\n",
    "    for lagIndex in range(order):\n",
    "        reducedXcomplete = np.hstack((reducedXcomplete, reducedX[lagIndex : samplesCount - order + lagIndex]))\n",
    "\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg_fit = reg.fit(completeX, causedNew)\n",
    "\n",
    "    predictedYcomplete = reg.predict(completeX)\n",
    "    predictedYcomplete = predictedYcomplete\n",
    "    errorComplete = causedNew - predictedYcomplete\n",
    "\n",
    "\n",
    "    regressionModel = linear_model.LinearRegression()\n",
    "    regressionFitReduced = regressionModel.fit(reducedXcomplete, causedNew)\n",
    "    regressionReduced = regressionModel.predict(reducedXcomplete)\n",
    "    errorReducedComplete = causedNew - regressionReduced\n",
    "\n",
    "    #error complete\n",
    "    residueComplete = np.sum(errorComplete**2)\n",
    "    \n",
    "    #reduced error\n",
    "    residueReducedComplete = np.sum(errorReducedComplete**2)\n",
    "    fValue = ((residueReducedComplete - residueComplete)/order) / (residueComplete / (samplesCount-order))\n",
    "    \n",
    "    gammaF = (order) * fValue\n",
    "\n",
    "    pValue = stats.distributions.chi2.sf(gammaF,order)\n",
    "    return fValue,pValue;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save utility functions\n",
    "def saveMetricData(metric,smoothWindowSize,lagDays,pDataPValue,pDataValue,gridRegions):\n",
    "    dataName = \"%s_d%d_l%d_nx%d_ny%d_R%d-%d_v8\"%(metric,smoothWindowSize,lagDays,nx,ny,dataRange[0],dataRange[1]);\n",
    "    fd = open(\"../Data/%s.csv\"%dataName,\"w\");\n",
    "    fd.write(\"lat,lng,pvalue,value\\n\");\n",
    "    for i,region in enumerate(gridRegions):\n",
    "        regionPvalue = pDataPValue[i];\n",
    "        regionValue = pDataValue[i];\n",
    "        lat,lng = (region[0]+region[1])*0.5,(region[2]+region[3])*0.5;\n",
    "        fd.write(\"%.20g,%.20g,%.20g,%.20g\\n\"%(lat,lng,regionPvalue,regionValue));\n",
    "    fd.close();"
   ]
  },
  {
   "source": [
    "Use the following code to calculate granger causality and lagged correlation for the the specified resolution range."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AllCalculation\n",
    "temperatures = temperaturesOrig[:,int(temperaturesOrig.shape[1]*dataRange[0]/100.0):int(temperaturesOrig.shape[1]*dataRange[1]/100.0)]\n",
    "preciptations = preciptationsOrig[:,int(preciptationsOrig.shape[1]*dataRange[0]/100.0):int(preciptationsOrig.shape[1]*dataRange[1]/100.0)]\n",
    "\n",
    "\n",
    "for smoothWindowSize in tqdm(resolutionRange):\n",
    "    # Setting the ENSO Region as starting point\n",
    "    regions = {\n",
    "       \"ENSO34\"     : [ -5  , 5, -170, -120],\n",
    "    }\n",
    "    fromRegionName = \"ENSO34\"\n",
    "\n",
    "    # Setting the window size to be 7 days\n",
    "    # and choosing maximum lat to 90 days\n",
    "    fromRegion = regions[fromRegionName]\n",
    "\n",
    "\n",
    "    gridRegions = [];\n",
    "    x = np.linspace(-180+180/(nx), 180+180/(nx), nx+1)\n",
    "    y = np.linspace(-70+70/(ny), 70+70/(ny), ny+1)\n",
    "    xv, yv = np.meshgrid(x, y,sparse=False, indexing='ij')\n",
    "    for xi in range(nx):\n",
    "        for yi in range(ny):\n",
    "            gridRegions.append([yv[xi,yi], yv[xi+1,yi+1], xv[xi,yi],xv[xi+1,yi+1]]);\n",
    "\n",
    "    lonArray = np.array(longitudes,dtype=float);\n",
    "    latArray = np.array(latitudes,dtype=float);\n",
    "    select180 = lonArray==-180;\n",
    "    lonArray = np.concatenate([lonArray,-lonArray[select180]])\n",
    "    latArray = np.concatenate([latArray,latArray[select180]])\n",
    "    latArray[np.abs(latArray)<10e-4] = 0\n",
    "    lonArray[np.abs(lonArray)<10e-4] = 0\n",
    "\n",
    "    temperaturesArray = np.concatenate([temperatures,temperatures[select180]])\n",
    "    preciptationsArray = np.concatenate([preciptations,preciptations[select180]])\n",
    "\n",
    "    inputTemperatures,errorsTemperatures = averageTimeseriesForRegions([fromRegion],temperaturesArray,lonArray,latArray,w=smoothWindowSize)\n",
    "    inputPreciptations,errorsPreciptations = averageTimeseriesForRegions(gridRegions,preciptationsArray,lonArray,latArray,w=smoothWindowSize)\n",
    "\n",
    "    seriesNames = [\"T(%s)\"%fromRegionName]+[\"P(%d)\"%index for index in range(len(gridRegions))]\n",
    "    # seriesNames.append(seriesNames[-1]+\"2\")\n",
    "    inputSeriesOriginal = np.array(inputTemperatures+inputPreciptations)\n",
    "    errorSeriesOriginal = np.array(errorsTemperatures+errorsPreciptations)\n",
    "\n",
    "    seriesStart = 0\n",
    "    seriesEnd = 20000;\n",
    "    seriesStep = smoothWindowSize;\n",
    "    inputSeries = np.array(inputSeriesOriginal[:,seriesStart:seriesEnd:seriesStep]);\n",
    "    errorSeries = np.array(errorSeriesOriginal[:,seriesStart:seriesEnd:seriesStep]);\n",
    "\n",
    "    yy = inputSeries[0];\n",
    "    xx = np.arange(0,len(yy))\n",
    "    xx = np.reshape(xx, (len(xx), 1))\n",
    "    selectedSeries = set(np.random.choice(np.arange(len(inputSeries)),10))\n",
    "    # fig = plt.figure(figsize=(9,2*(len(selectedSeries)+1)),tight_layout=True);\n",
    "\n",
    "    # detrending time series by using linear regression\n",
    "    plotIndex=0;\n",
    "    for i,yvalues in enumerate(inputSeries):\n",
    "        xvalues = np.arange(0,len(inputSeries[i]));\n",
    "        xx = np.reshape(xvalues, (len(xvalues), 1))\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(xx, yvalues)\n",
    "        trend = model.predict(xx)\n",
    "    #     model = make_pipeline(PolynomialFeatures(2), Ridge())\n",
    "        if(i in selectedSeries or i==0):\n",
    "    #         plt.subplot(len(selectedSeries)+1,1,plotIndex+1);\n",
    "    #         plt.plot(xvalues,yvalues,label = seriesNames[i],c=\"#333333\",linewidth=0.5);\n",
    "    #         plt.fill_between(xvalues,inputSeries[i]-errorSeries[i],inputSeries[i]+errorSeries[i],alpha=0.2,color=\"#ff3333\")\n",
    "    #         plt.plot(xvalues,trend,c=\"#33ff33\");\n",
    "    #         plt.legend(loc=2,fancybox=False);\n",
    "            plotIndex+=1;\n",
    "        inputSeries[i,:] = yvalues-trend;\n",
    "    # plt.savefig(\"series.pdf\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "#     # Calculating Granger Causality between first time series and all others\n",
    "    order = int(math.ceil(lagDays/seriesStep)) # Redefines max lag in terms of the chosen window size \n",
    "    grangerXY = []\n",
    "    grangerPValuesXY = [];\n",
    "    for i in tqdm(range(1,len(inputSeries))):\n",
    "        X = inputSeries[0];\n",
    "        Y = inputSeries[i];\n",
    "        grangerValue,pValue = calcCausalityBivariate(np.array([X,Y]).T, 1, 0, order);\n",
    "        grangerXY.append(grangerValue)\n",
    "        grangerPValuesXY.append(pValue)\n",
    "\n",
    "#     Save data\n",
    "    saveMetricData(\"Causality\",smoothWindowSize,lagDays,np.array(grangerPValuesXY),np.abs(grangerXY),gridRegions=gridRegions);\n",
    "\n",
    "    \n",
    "    # Computing correlation p-values for the pairwise model\n",
    "    inputSurrogates = pysurrogates.Surrogates(inputSeries,silence_level=2);\n",
    "\n",
    "    correlationValues,lags = computeLagCorrelations(inputSeries[0], inputSeries[1:], order);\n",
    "    nullCount = 10000\n",
    "    distribs = np.zeros((nullCount,inputSeries.shape[0]-1));\n",
    "    for i in tqdm(range(nullCount)):\n",
    "        surrogatesSeries = inputSurrogates.correlated_noise_surrogates(inputSeries)\n",
    "        correlationValuesSurrogate,lagsSurrogate = computeLagCorrelations(surrogatesSeries[0], surrogatesSeries[1:], order);\n",
    "        distribs[i] = correlationValuesSurrogate;\n",
    "\n",
    "    laggedValuesXY = np.abs(correlationValues);\n",
    "    laggedPValuesXY = np.zeros(len(correlationValues))\n",
    "    absDistribs = np.abs(distribs)\n",
    "    for i in range(len(laggedValuesXY)):\n",
    "        laggedDistrib = absDistribs[:,i];\n",
    "        laggedPValuesXY[i] = np.sum(laggedDistrib>=laggedValuesXY[i])/len(laggedDistrib);\n",
    "\n",
    "    saveMetricData(\"LaggedCorrelation\",smoothWindowSize,lagDays,np.array(laggedPValuesXY),np.abs(laggedPValuesXY),gridRegions=gridRegions);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation and best lag distributions for the general model\n",
    "plt.close(\"all\")\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(10,4))\n",
    "bins = np.linspace(0.0,0.3,100);\n",
    "ax1.hist(np.abs(distribsGeneral),bins=bins,color=\"#999999\",alpha=0.5,density=True,label=\"Null model\");\n",
    "ax1.set_xlim(0,0.4)\n",
    "ax1.hist(np.abs(correlationValues),bins=bins,alpha=0.5,density=True,label=\"Data\");\n",
    "ax1.set_xlabel(\"Lagged correlation\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"Lagged correlation distribution\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(lagsSurrogate,bins=np.arange(0,order+1),color=\"#999999\",alpha=0.5,density=True,label=\"Null model\");\n",
    "ax2.hist(lags,bins=np.arange(0,order+1),alpha=0.5,density=True,label=\"Data\");\n",
    "ax2.set_xlabel(\"Best lag\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax2.set_title(\"Best lag distribution\")\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Lagged correlation and best lags distributions\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(10,4))\n",
    "bins = np.linspace(0.0,0.3,100);\n",
    "ax1.hist(np.abs(distribs.flatten()),bins=bins,color=\"#999999\",alpha=0.1,density=True,label=\"Null model\");\n",
    "\n",
    "distribAverages = np.average(np.abs(distribs),axis=0);\n",
    "distribStds = np.std(np.abs(distribs),axis=0);\n",
    "top = np.argmax(distribAverages)\n",
    "bottom = np.argmin(distribAverages);\n",
    "ax1.axvline(np.abs(correlationValues)[top],color=\"#bb0000\")\n",
    "ax1.hist(np.abs(distribs[:,top]),bins=bins,color=\"#bb0000\",alpha=0.3,density=True,label=\"Highest Avg. P\");\n",
    "ax1.axvline(np.abs(correlationValues)[bottom],color =\"#00bb00\" )\n",
    "ax1.hist(np.abs(distribs[:,bottom]),bins=bins,color=\"#00bb00\",alpha=0.3,density=True,label=\"Lowest Avg. P\");\n",
    "ax1.set_xlim(0,0.4)\n",
    "ax1.hist(np.abs(correlationValues),bins=bins,alpha=0.5,density=True,label=\"Data\");\n",
    "ax1.set_xlabel(\"Lagged Correlation\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"Lagged correlation distribution\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(lagsSurrogate,bins=np.arange(0,order+1),color=\"#999999\",density=True);\n",
    "ax2.hist(lags,bins=np.arange(0,order+1),alpha=0.5,density=True);\n",
    "ax2.set_xlabel(\"Best lag\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax2.set_title(\"Best lag distribution\")\n",
    "# plt.savefig(\"Figures/CorrelationWithLag.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# fig, (ax1) = plt.subplots(1, 1)\n",
    "# ax1.scatter(lags,correlationValues,alpha=0.02);\n",
    "# ax1.set_xlabel(\"Corr.\")\n",
    "# ax1.set_ylabel(\"Lag\")\n",
    "# plt.show()\n",
    "# plt.savefig(\"Figures/CorrelationVsWithLag.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd06b72e51acbe6c3342e47b10d22082aac1e4bc0774fa929de3d309638ced4be6e",
   "display_name": "Python 3.8.6 64-bit ('teleconnectionsgranger': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
